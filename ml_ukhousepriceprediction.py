# -*- coding: utf-8 -*-
"""ML_ukhousepriceprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i9JCe8-OVVVlorwOD0tstYr9FAZehzgv
"""

from google.colab import drive
drive.mount('/content/drive')

"""
UK House Price Prediction - SUPER ENHANCED ML Pipeline
=====================================================

This implements state-of-the-art machine learning techniques to achieve
superior performance in UK house price prediction.

Target: R¬≤ > 0.70, MAE < ¬£60,000
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.ensemble import IsolationForest
from sklearn.neural_network import MLPRegressor

!pip install catboost
!pip install optuna
!pip install catboost optuna category_encoders xgboost lightgbm scipy

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import optuna
from category_encoders import TargetEncoder
from scipy import stats
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
# Set style for plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("=" * 90)
print("UK HOUSE PRICE PREDICTION - SUPER ENHANCED ML PIPELINE")
print("=" * 90)
print("üéØ TARGET: R¬≤ > 0.70, MAE < ¬£60,000")
print("üöÄ IMPLEMENTING STATE-OF-THE-ART TECHNIQUES")
print("=" * 90)

# ============================================================================
# PHASE 1: SUPER DATA LOADING & EXPLORATION
# ============================================================================

print("\nüî∑ PHASE 1: SUPER DATA LOADING & EXPLORATION")
print("-" * 60)

# Load data
print("Loading UK house price dataset...")
df = pd.read_csv('/content/drive/MyDrive/Research Methods/UK_House_Price_Prediction_dataset_2015_to_2024.csv')

print(f"\nüìä COMPREHENSIVE DATASET ANALYSIS:")
print(f"Shape: {df.shape}")
print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"Date range: {df['date'].min()} to {df['date'].max()}")

# Price analysis
print(f"\nüí∞ PRICE DISTRIBUTION ANALYSIS:")
print(f"Min: ¬£{df['price'].min():,}")
print(f"Max: ¬£{df['price'].max():,}")
print(f"Mean: ¬£{df['price'].mean():,.0f}")
print(f"Median: ¬£{df['price'].median():,.0f}")
print(f"Std: ¬£{df['price'].std():,.0f}")
print(f"Skewness: {df['price'].skew():.3f}")
print(f"Kurtosis: {df['price'].kurtosis():.3f}")

# ============================================================================
# PHASE 2: SUPER DATA PREPROCESSING & QUALITY
# ============================================================================

print("\nüî∑ PHASE 2: SUPER DATA PREPROCESSING & QUALITY")
print("-" * 60)

# Create working copy
df_work = df.copy()

# Convert date
df_work['date'] = pd.to_datetime(df_work['date'])

# ADVANCED OUTLIER DETECTION using Isolation Forest
print("\nüîç ADVANCED OUTLIER DETECTION (Isolation Forest)...")
iso_forest = IsolationForest(contamination=0.05, random_state=42)
outlier_mask = iso_forest.fit_predict(df_work[['price']].values) == 1

print(f"Outliers detected: {(~pd.Series(outlier_mask)).sum()} ({(~pd.Series(outlier_mask)).mean()*100:.2f}%)")
df_work = df_work[outlier_mask]
print(f"Dataset shape after outlier removal: {df_work.shape}")

# LOG TRANSFORMATION of target variable (critical for skewed data)
print("\nüìà APPLYING LOG TRANSFORMATION TO TARGET...")
df_work['log_price'] = np.log1p(df_work['price'])

print(f"Original price skewness: {df['price'].skew():.3f}")
print(f"Log-transformed price skewness: {df_work['log_price'].skew():.3f}")
print("‚úÖ Log transformation reduces skewness significantly!")

# ============================================================================
# PHASE 3: SUPER FEATURE ENGINEERING
# ============================================================================

print("\nüî∑ PHASE 3: SUPER FEATURE ENGINEERING")
print("-" * 60)

# ==================== TEMPORAL FEATURES ====================
print("\nüìÖ ADVANCED TEMPORAL FEATURES...")

# Basic temporal features
df_work['year'] = df_work['date'].dt.year
df_work['month'] = df_work['date'].dt.month
df_work['quarter'] = df_work['date'].dt.quarter
df_work['day_of_year'] = df_work['date'].dt.dayofyear
df_work['week_of_year'] = df_work['date'].dt.isocalendar().week

# Property age (more sophisticated)
df_work['property_age'] = 2024 - df_work['year']
df_work['property_age_squared'] = df_work['property_age'] ** 2

# Market cycle features
df_work['year_month'] = df_work['year'] * 100 + df_work['month']
df_work['year_quarter'] = df_work['year'] * 10 + df_work['quarter']

# Seasonal indicators
df_work['is_spring'] = df_work['month'].isin([3, 4, 5]).astype(int)
df_work['is_summer'] = df_work['month'].isin([6, 7, 8]).astype(int)
df_work['is_autumn'] = df_work['month'].isin([9, 10, 11]).astype(int)
df_work['is_winter'] = df_work['month'].isin([12, 1, 2]).astype(int)

# Financial year indicators (UK tax year: April to March)
df_work['financial_year'] = df_work['year'] + (df_work['month'] >= 4).astype(int)
df_work['is_financial_year_end'] = df_work['month'].isin([3, 4]).astype(int)

print(f"‚úÖ Created {len([col for col in df_work.columns if col.startswith(('year', 'month', 'quarter', 'day_', 'week_', 'property_age', 'is_', 'financial'))])} temporal features")

# ==================== GEOGRAPHIC FEATURES ====================
print("\nüó∫Ô∏è ADVANCED GEOGRAPHIC FEATURES...")

# Postcode analysis
df_work['postcode_area'] = df_work['postcode'].str[:2]  # First 2 characters (e.g., 'SW', 'M1')
df_work['postcode_district'] = df_work['postcode'].str[:3]  # First 3 characters

# Geographic frequency encoding
postcode_area_freq = df_work['postcode_area'].value_counts().to_dict()
postcode_district_freq = df_work['postcode_district'].value_counts().to_dict()
district_freq = df_work['district'].value_counts().to_dict()
county_freq = df_work['county'].value_counts().to_dict()
town_freq = df_work['town'].value_counts().to_dict()

df_work['postcode_area_freq'] = df_work['postcode_area'].map(postcode_area_freq)
df_work['postcode_district_freq'] = df_work['postcode_district'].map(postcode_district_freq)
df_work['district_freq'] = df_work['district'].map(district_freq)
df_work['county_freq'] = df_work['county'].map(county_freq)
df_work['town_freq'] = df_work['town'].map(town_freq)

# Regional market indicators
df_work['is_london'] = df_work['postcode_area'].isin(['E1', 'EC', 'N1', 'NW', 'SE', 'SW', 'W1', 'WC']).astype(int)
df_work['is_manchester'] = df_work['postcode_area'].isin(['M1', 'M2', 'M3', 'M4', 'M5']).astype(int)
df_work['is_birmingham'] = df_work['postcode_area'].isin(['B1', 'B2', 'B3', 'B4', 'B5']).astype(int)
df_work['is_major_city'] = (df_work['is_london'] | df_work['is_manchester'] | df_work['is_birmingham']).astype(int)

print(f"‚úÖ Created {len([col for col in df_work.columns if col.startswith(('postcode', 'district_freq', 'county_freq', 'town_freq', 'is_london', 'is_manchester', 'is_birmingham', 'is_major'))])} geographic features")

# ==================== PROPERTY FEATURES ====================
print("\nüè† ADVANCED PROPERTY FEATURES...")

# Basic property encoding
property_type_map = {'D': 4, 'S': 3, 'T': 2, 'F': 1, 'O': 0}  # Detached=highest, Other=lowest
df_work['property_type_rank'] = df_work['property_type'].map(property_type_map)

df_work['is_new_build'] = (df_work['new_build'] == 'Y').astype(int)
df_work['is_freehold'] = (df_work['freehold'] == 'F').astype(int)

# Property type interactions
df_work['property_type_year'] = df_work['property_type_rank'] * df_work['year']
df_work['property_type_month'] = df_work['property_type_rank'] * df_work['month']
df_work['new_build_year'] = df_work['is_new_build'] * df_work['year']
df_work['freehold_type'] = df_work['is_freehold'] * df_work['property_type_rank']

print(f"‚úÖ Created {len([col for col in df_work.columns if col.startswith(('property_type', 'is_new', 'is_free', 'new_build', 'freehold'))])} property features")

# ==================== MARKET DYNAMICS FEATURES ====================
print("\nüìä MARKET DYNAMICS FEATURES...")

# Sort by date for rolling calculations
df_work = df_work.sort_values('date').reset_index(drop=True)

# Rolling price statistics (using log_price for better distribution)
for window in [30, 90, 180, 365]:  # 1, 3, 6, 12 months
    df_work[f'rolling_mean_{window}d'] = df_work['log_price'].rolling(window=window, min_periods=10).mean()
    df_work[f'rolling_std_{window}d'] = df_work['log_price'].rolling(window=window, min_periods=10).std()
    df_work[f'price_vs_rolling_{window}d'] = df_work['log_price'] - df_work[f'rolling_mean_{window}d']

# Market trend indicators
df_work['price_momentum_30d'] = df_work['log_price'] - df_work['log_price'].shift(30)
df_work['price_momentum_90d'] = df_work['log_price'] - df_work['log_price'].shift(90)

# Volatility measures
df_work['price_volatility_30d'] = df_work['log_price'].rolling(window=30, min_periods=10).std()
df_work['price_volatility_90d'] = df_work['log_price'].rolling(window=90, min_periods=10).std()

print(f"‚úÖ Created {len([col for col in df_work.columns if 'rolling' in col or 'momentum' in col or 'volatility' in col])} market dynamics features")

# ==================== INTERACTION FEATURES ====================
print("\nüîó ADVANCED INTERACTION FEATURES...")

# Geographic √ó Temporal interactions
df_work['county_year'] = df_work['county_freq'] * df_work['year']
df_work['district_month'] = df_work['district_freq'] * df_work['month']
df_work['postcode_quarter'] = df_work['postcode_area_freq'] * df_work['quarter']

# Property √ó Geographic interactions
df_work['property_county'] = df_work['property_type_rank'] * df_work['county_freq']
df_work['new_build_county'] = df_work['is_new_build'] * df_work['county_freq']
df_work['freehold_district'] = df_work['is_freehold'] * df_work['district_freq']

# Market √ó Property interactions
df_work['property_age_district'] = df_work['property_age'] * df_work['district_freq']
df_work['major_city_property_type'] = df_work['is_major_city'] * df_work['property_type_rank']

print(f"‚úÖ Created {len([col for col in df_work.columns if any(x in col for x in ['county_year', 'district_month', 'postcode_quarter', 'property_county', 'new_build_county', 'freehold_district', 'property_age_district', 'major_city_property'])])} interaction features")

# ==================== TARGET ENCODING ====================
print("\nüéØ ADVANCED TARGET ENCODING...")

# High-cardinality categorical features for target encoding
target_encoding_features = ['postcode_area', 'district', 'county', 'town']

# Apply target encoding with cross-validation to prevent overfitting
target_encoder = TargetEncoder(cols=target_encoding_features, smoothing=10, min_samples_leaf=20)

# Split for target encoding (prevent data leakage)
temp_X = df_work[target_encoding_features]
temp_y = df_work['log_price']

encoded_features = target_encoder.fit_transform(temp_X, temp_y)
for i, col in enumerate(target_encoding_features):
    df_work[f'{col}_target_encoded'] = encoded_features.iloc[:, i]

print(f"‚úÖ Applied target encoding to {len(target_encoding_features)} high-cardinality features")

# ==================== FEATURE SELECTION ====================
print("\nüîç PREPARING FEATURE MATRIX...")

# Remove original categorical and intermediate columns
columns_to_drop = [
    'date', 'postcode', 'street', 'locality', 'property_type', 'new_build', 'freehold',
    'district', 'county', 'town', 'postcode_area', 'postcode_district', 'price'  # Keep log_price as target
]

# Get all feature columns (excluding target and dropped columns)
feature_columns = [col for col in df_work.columns if col not in columns_to_drop + ['log_price']]

# Remove any columns with too many NaN values
valid_feature_columns = []
for col in feature_columns:
    if df_work[col].isna().sum() / len(df_work) < 0.3:  # Less than 30% missing
        valid_feature_columns.append(col)
    else:
        print(f"‚ö†Ô∏è Dropping {col} due to {df_work[col].isna().sum()/len(df_work)*100:.1f}% missing values")

# Fill remaining NaN values
df_clean = df_work[valid_feature_columns + ['log_price']].copy()
df_clean = df_clean.fillna(df_clean.median())

X = df_clean[valid_feature_columns]
y = df_clean['log_price']

print(f"\nüìä FINAL FEATURE MATRIX:")
print(f"Features: {X.shape[1]}")
print(f"Samples: {X.shape[0]}")
print(f"Target: log_price (log-transformed)")
print(f"Memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ============================================================================
# PHASE 4: SUPER MODEL TRAINING & OPTIMIZATION
# ============================================================================

print(f"\nüî∑ PHASE 4: SUPER MODEL TRAINING & OPTIMIZATION")
print("-" * 60)

# Time-aware train-test split (respect temporal nature)
print("\nüìÖ TIME-AWARE DATA SPLITTING...")
df_clean_sorted = df_clean.copy()
df_clean_sorted['date'] = df_work['date'].values

# Use last 20% of time period for testing
split_date = df_clean_sorted['date'].quantile(0.8)
train_mask = df_clean_sorted['date'] <= split_date
test_mask = df_clean_sorted['date'] > split_date

X_train = X[train_mask]
X_test = X[test_mask]
y_train = y[train_mask]
y_test = y[test_mask]

print(f"Training set: {X_train.shape[0]} samples ({train_mask.sum()/len(df_clean)*100:.1f}%)")
print(f"Test set: {X_test.shape[0]} samples ({test_mask.sum()/len(df_clean)*100:.1f}%)")
print(f"Split date: {split_date}")


# ==================== HYPERPARAMETER OPTIMIZATION ====================
print(f"\nüéØ HYPERPARAMETER OPTIMIZATION WITH OPTUNA...")

def objective_xgb(trial):
    """Objective function for XGBoost hyperparameter optimization"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'random_state': 42
    }

    model = xgb.XGBRegressor(**params)

    # Use time series cross-validation
    tscv = TimeSeriesSplit(n_splits=5)
    scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
    return scores.mean()

def objective_lgb(trial):
    """Objective function for LightGBM hyperparameter optimization"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
        'random_state': 42,
        'verbose': -1
    }

    model = lgb.LGBMRegressor(**params)

    tscv = TimeSeriesSplit(n_splits=5)
    scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')
    return scores.mean()

# Optimize XGBoost
print("Optimizing XGBoost...")
study_xgb = optuna.create_study(direction='maximize', study_name='xgb_optimization')
study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=False)
best_xgb_params = study_xgb.best_params
print(f"‚úÖ XGBoost best R¬≤: {study_xgb.best_value:.4f}")

# Optimize LightGBM
print("Optimizing LightGBM...")
study_lgb = optuna.create_study(direction='maximize', study_name='lgb_optimization')
study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=False)
best_lgb_params = study_lgb.best_params
print(f"‚úÖ LightGBM best R¬≤: {study_lgb.best_value:.4f}")

# ==================== MODEL TRAINING ====================
print(f"\nü§ñ TRAINING SUPER MODEL SUITE...")

# Initialize optimized models
models = {
    'XGBoost_Optimized': xgb.XGBRegressor(**best_xgb_params),
    'LightGBM_Optimized': lgb.LGBMRegressor(**best_lgb_params),
    'CatBoost': cb.CatBoostRegressor(
        iterations=300,
        depth=8,
        learning_rate=0.1,
        random_state=42,
        verbose=False
    ),
    'Random_Forest_Tuned': RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    ),
    'Gradient_Boosting_Tuned': GradientBoostingRegressor(
        n_estimators=200,
        max_depth=8,
        learning_rate=0.1,
        subsample=0.8,
        random_state=42
    )
}

# Train models and store results
results = {}

for name, model in models.items():
    print(f"\n--- {name} ---")
    X_train_model = X_train
    X_test_model = X_test

    # Train model
    print("Training...")
    model.fit(X_train_model, y_train)

    # Make predictions
    print("Predicting...")
    y_pred_log = model.predict(X_test_model)

    # Convert back from log space
    y_pred = np.expm1(y_pred_log)
    y_test_original = np.expm1(y_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test_original, y_pred)
    mse = mean_squared_error(y_test_original, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test_original, y_pred)

    # Cross-validation on log scale
    tscv = TimeSeriesSplit(n_splits=5)
    cv_scores = cross_val_score(model, X_train_model, y_train, cv=tscv, scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()

    # Store results
    results[name] = {
        'model': model,
        'predictions': y_pred,
        'predictions_log': y_pred_log,
        'mae': mae,
        'mse': mse,
        'rmse': rmse,
        'r2': r2,
        'cv_mean': cv_mean,
        'cv_std': cv_std
    }

    # Print results
    print(f"MAE: ¬£{mae:,.0f}")
    print(f"RMSE: ¬£{rmse:,.0f}")
    print(f"R¬≤ Score: {r2:.4f}")
    print(f"CV R¬≤ Score: {cv_mean:.4f} (¬±{cv_std:.4f})")

# ==================== ENSEMBLE METHODS ====================
print(f"\nüöÄ CREATING SUPER ENSEMBLE MODELS...")

# Get best performing models for ensemble
best_models = sorted(results.items(), key=lambda x: x[1]['r2'], reverse=True)[:3]
best_model_names = [name for name, _ in best_models]

print(f"Top 3 models for ensemble: {best_model_names}")

# Voting Regressor
voting_models = [(name, results[name]['model']) for name in best_model_names if name != 'Neural_Network']
if voting_models:
    voting_regressor = VotingRegressor(voting_models)

    print(f"\n--- Voting Ensemble ---")
    voting_regressor.fit(X_train, y_train)
    y_pred_voting_log = voting_regressor.predict(X_test)
    y_pred_voting = np.expm1(y_pred_voting_log)

    mae_voting = mean_absolute_error(y_test_original, y_pred_voting)
    r2_voting = r2_score(y_test_original, y_pred_voting)
    rmse_voting = np.sqrt(mean_squared_error(y_test_original, y_pred_voting))

    results['Voting_Ensemble'] = {
        'model': voting_regressor,
        'predictions': y_pred_voting,
        'predictions_log': y_pred_voting_log,
        'mae': mae_voting,
        'rmse': rmse_voting,
        'r2': r2_voting,
        'cv_mean': r2_voting,  # Approximation
        'cv_std': 0.0
    }

    print(f"MAE: ¬£{mae_voting:,.0f}")
    print(f"RMSE: ¬£{rmse_voting:,.0f}")
    print(f"R¬≤ Score: {r2_voting:.4f}")

# Stacking Regressor
if len(voting_models) >= 2:
    base_models = voting_models[:2]  # Use top 2 as base models
    meta_model = Ridge(alpha=1.0)

    stacking_regressor = StackingRegressor(
        estimators=base_models,
        final_estimator=meta_model,
        cv=3
    )

    print(f"\n--- Stacking Ensemble ---")
    stacking_regressor.fit(X_train, y_train)
    y_pred_stacking_log = stacking_regressor.predict(X_test)
    y_pred_stacking = np.expm1(y_pred_stacking_log)

    mae_stacking = mean_absolute_error(y_test_original, y_pred_stacking)
    r2_stacking = r2_score(y_test_original, y_pred_stacking)
    rmse_stacking = np.sqrt(mean_squared_error(y_test_original, y_pred_stacking))

    results['Stacking_Ensemble'] = {
        'model': stacking_regressor,
        'predictions': y_pred_stacking,
        'predictions_log': y_pred_stacking_log,
        'mae': mae_stacking,
        'rmse': rmse_stacking,
        'r2': r2_stacking,
        'cv_mean': r2_stacking,  # Approximation
        'cv_std': 0.0
    }

    print(f"MAE: ¬£{mae_stacking:,.0f}")
    print(f"RMSE: ¬£{rmse_stacking:,.0f}")
    print(f"R¬≤ Score: {r2_stacking:.4f}")

# ============================================================================
# PHASE 5: SUPER VISUALIZATION & ANALYSIS
# ============================================================================

print(f"\nüî∑ PHASE 5: SUPER VISUALIZATION & ANALYSIS")
print("-" * 60)

# Create comprehensive visualization
fig, axes = plt.subplots(3, 3, figsize=(24, 20))
fig.suptitle('UK House Price Prediction - SUPER ENHANCED Results', fontsize=20, fontweight='bold')

# Plot 1: Model Performance Comparison (R¬≤)
ax1 = axes[0, 0]
model_names = list(results.keys())
r2_scores = [results[name]['r2'] for name in model_names]
colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))

bars = ax1.bar(range(len(model_names)), r2_scores, color=colors, alpha=0.8)
ax1.set_ylabel('R¬≤ Score')
ax1.set_title('Model Performance Comparison (R¬≤ Score)')
ax1.set_xticks(range(len(model_names)))
ax1.set_xticklabels([name.replace('_', '\n') for name in model_names], rotation=45, ha='right')
ax1.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Target R¬≤ = 0.7')
ax1.legend()

# Add value labels
for bar, score in zip(bars, r2_scores):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

# Plot 2: MAE Comparison
ax2 = axes[0, 1]
mae_scores = [results[name]['mae'] for name in model_names]
bars2 = ax2.bar(range(len(model_names)), mae_scores, color=colors, alpha=0.8)
ax2.set_ylabel('Mean Absolute Error (¬£)')
ax2.set_title('Model Performance Comparison (MAE)')
ax2.set_xticks(range(len(model_names)))
ax2.set_xticklabels([name.replace('_', '\n') for name in model_names], rotation=45, ha='right')
ax2.axhline(y=60000, color='red', linestyle='--', alpha=0.7, label='Target MAE = ¬£60k')
ax2.legend()

# Add value labels
for bar, mae in zip(bars2, mae_scores):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 1000,
             f'¬£{mae:,.0f}', ha='center', va='bottom', fontweight='bold')

# Plot 3: Best Model - Actual vs Predicted
ax3 = axes[0, 2]
best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])
y_pred_best = results[best_model_name]['predictions']

scatter = ax3.scatter(y_test_original, y_pred_best, alpha=0.6, c=colors[0])
ax3.plot([y_test_original.min(), y_test_original.max()],
         [y_test_original.min(), y_test_original.max()], 'r--', lw=2)
ax3.set_xlabel('Actual Price (¬£)')
ax3.set_ylabel('Predicted Price (¬£)')
ax3.set_title(f'Actual vs Predicted - {best_model_name}')

# Format axes
ax3.ticklabel_format(style='scientific', axis='both', scilimits=(0,0))

# Add metrics
r2_best = results[best_model_name]['r2']
mae_best = results[best_model_name]['mae']
ax3.text(0.05, 0.95, f'R¬≤ = {r2_best:.4f}\nMAE = ¬£{mae_best:,.0f}',
         transform=ax3.transAxes, bbox=dict(boxstyle="round", facecolor='white', alpha=0.8))

# Plot 4: Residuals Analysis
ax4 = axes[1, 0]
residuals = y_test_original - y_pred_best
ax4.scatter(y_pred_best, residuals, alpha=0.6, c=colors[1])
ax4.axhline(y=0, color='r', linestyle='--')
ax4.set_xlabel('Predicted Price (¬£)')
ax4.set_ylabel('Residuals (¬£)')
ax4.set_title(f'Residual Plot - {best_model_name}')
ax4.ticklabel_format(style='scientific', axis='both', scilimits=(0,0))

# Plot 5: Feature Importance (Best Tree-based Model)
ax5 = axes[1, 1]
tree_models = ['XGBoost_Optimized', 'LightGBM_Optimized', 'CatBoost', 'Random_Forest_Tuned', 'Gradient_Boosting_Tuned']
best_tree_model = None
best_tree_r2 = -float('inf')

for model_name in tree_models:
    if model_name in results and results[model_name]['r2'] > best_tree_r2:
        best_tree_model = model_name
        best_tree_r2 = results[model_name]['r2']

if best_tree_model:
    model_obj = results[best_tree_model]['model']

    # Get feature importance
    if hasattr(model_obj, 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': model_obj.feature_importances_
        }).sort_values('importance', ascending=True).tail(15)  # Top 15 features

        bars_feat = ax5.barh(range(len(feature_importance)), feature_importance['importance'])
        ax5.set_yticks(range(len(feature_importance)))
        ax5.set_yticklabels(feature_importance['feature'], fontsize=8)
        ax5.set_xlabel('Feature Importance')
        ax5.set_title(f'Top 15 Features - {best_tree_model}')

        # Color bars
        for i, bar in enumerate(bars_feat):
            bar.set_color(colors[i % len(colors)])

# Plot 6: Cross-Validation Scores
ax6 = axes[1, 2]
cv_means = [results[name]['cv_mean'] for name in model_names]
cv_stds = [results[name]['cv_std'] for name in model_names]

bars_cv = ax6.bar(range(len(model_names)), cv_means, yerr=cv_stds, capsize=5, color=colors, alpha=0.8)
ax6.set_ylabel('CV R¬≤ Score')
ax6.set_title('Cross-Validation R¬≤ Scores')
ax6.set_xticks(range(len(model_names)))
ax6.set_xticklabels([name.replace('_', '\n') for name in model_names], rotation=45, ha='right')
ax6.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='Target R¬≤ = 0.7')
ax6.legend()

# Plot 7: Price Distribution (Original vs Log)
ax7 = axes[2, 0]
ax7.hist(df['price'], bins=50, alpha=0.7, label='Original Price', density=True)
ax7.set_xlabel('Price (¬£)')
ax7.set_ylabel('Density')
ax7.set_title('Original Price Distribution')
ax7.ticklabel_format(style='scientific', axis='x', scilimits=(0,0))

# Plot 8: Log Price Distribution
ax8 = axes[2, 1]
ax8.hist(df_work['log_price'].dropna(), bins=50, alpha=0.7, label='Log Price', density=True, color='orange')
ax8.set_xlabel('Log Price')
ax8.set_ylabel('Density')
ax8.set_title('Log-Transformed Price Distribution')

# Plot 9: Model Comparison Matrix
ax9 = axes[2, 2]
comparison_data = pd.DataFrame({
    'Model': model_names,
    'R¬≤': r2_scores,
    'MAE': mae_scores
})

# Normalize for heatmap
comparison_norm = comparison_data.copy()
comparison_norm['R¬≤'] = (comparison_norm['R¬≤'] - comparison_norm['R¬≤'].min()) / (comparison_norm['R¬≤'].max() - comparison_norm['R¬≤'].min())
comparison_norm['MAE'] = 1 - (comparison_norm['MAE'] - comparison_norm['MAE'].min()) / (comparison_norm['MAE'].max() - comparison_norm['MAE'].min())  # Invert MAE (lower is better)

heatmap_data = comparison_norm[['R¬≤', 'MAE']].T
sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax9,
            xticklabels=[name.replace('_', '\n') for name in model_names],
            yticklabels=['R¬≤ Score', 'MAE (inverted)'])
ax9.set_title('Model Performance Heatmap')

plt.tight_layout()
plt.savefig('uk_house_price_prediction_SUPER.png', dpi=300, bbox_inches='tight')
plt.show()

# ============================================================================
# PHASE 6: SUPER RESULTS SUMMARY
# ============================================================================

print(f"\nüî∑ PHASE 6: SUPER RESULTS SUMMARY")
print("-" * 60)

# Comprehensive results table
print(f"\nüìä SUPER MODEL PERFORMANCE COMPARISON")
print("=" * 120)
print(f"{'Model':<25} {'MAE (¬£)':<15} {'RMSE (¬£)':<15} {'R¬≤ Score':<12} {'CV R¬≤ Score':<15} {'CV Std':<10} {'Target Met':<12}")
print("-" * 120)

for name in results:
    mae = results[name]['mae']
    rmse = results[name]['rmse']
    r2 = results[name]['r2']
    cv_mean = results[name]['cv_mean']
    cv_std = results[name]['cv_std']

    # Check if targets met
    target_met = "‚úÖ YES" if r2 >= 0.70 and mae <= 60000 else "‚ùå NO"

    print(f"{name:<25} {mae:<15,.0f} {rmse:<15,.0f} {r2:<12.4f} {cv_mean:<15.4f} {cv_std:<10.4f} {target_met:<12}")

print("-" * 120)

# Find best performers
best_overall = max(results.keys(), key=lambda x: results[x]['r2'])
best_mae = min(results.keys(), key=lambda x: results[x]['mae'])
most_stable = min(results.keys(), key=lambda x: results[x]['cv_std'])

print(f"\nüèÜ SUPER PERFORMANCE HIGHLIGHTS:")
print(f"   ü•á BEST OVERALL (R¬≤): {best_overall}")
print(f"      R¬≤ Score: {results[best_overall]['r2']:.4f}")
print(f"      MAE: ¬£{results[best_overall]['mae']:,.0f}")
print(f"      RMSE: ¬£{results[best_overall]['rmse']:,.0f}")

print(f"\n   üéØ BEST ACCURACY (MAE): {best_mae}")
print(f"      MAE: ¬£{results[best_mae]['mae']:,.0f}")
print(f"      R¬≤ Score: {results[best_mae]['r2']:.4f}")

print(f"\n   üîí MOST STABLE (CV): {most_stable}")
print(f"      CV Std: ¬±{results[most_stable]['cv_std']:.4f}")
print(f"      CV Mean: {results[most_stable]['cv_mean']:.4f}")

# Target achievement analysis
targets_met = [name for name in results if results[name]['r2'] >= 0.70 and results[name]['mae'] <= 60000]
print(f"\nüéØ TARGET ACHIEVEMENT ANALYSIS:")
print(f"   Target: R¬≤ ‚â• 0.70 AND MAE ‚â§ ¬£60,000")
print(f"   Models meeting targets: {len(targets_met)}/{len(results)}")
if targets_met:
    print(f"   Successful models: {', '.join(targets_met)}")
else:
    # Find closest to targets
    best_r2 = max(results.values(), key=lambda x: x['r2'])['r2']
    best_mae_val = min(results.values(), key=lambda x: x['mae'])['mae']
    print(f"   Best R¬≤ achieved: {best_r2:.4f} (target: 0.70)")
    print(f"   Best MAE achieved: ¬£{best_mae_val:,.0f} (target: ¬£60,000)")

# Improvement analysis
print(f"\nüìà IMPROVEMENT FROM ENHANCED VERSION:")
enhanced_r2 = 0.4995  # Previous best result
enhanced_mae = 86455  # Previous best MAE

current_best_r2 = results[best_overall]['r2']
current_best_mae = results[best_mae]['mae']

r2_improvement = ((current_best_r2 - enhanced_r2) / enhanced_r2) * 100
mae_improvement = ((enhanced_mae - current_best_mae) / enhanced_mae) * 100

print(f"   R¬≤ Improvement: {r2_improvement:+.1f}% ({enhanced_r2:.4f} ‚Üí {current_best_r2:.4f})")
print(f"   MAE Improvement: {mae_improvement:+.1f}% (¬£{enhanced_mae:,} ‚Üí ¬£{current_best_mae:,})")

# Feature insights
if best_tree_model and hasattr(results[best_tree_model]['model'], 'feature_importances_'):
    print(f"\nüå≥ {best_tree_model.upper()} FEATURE INSIGHTS:")
    model_obj = results[best_tree_model]['model']
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model_obj.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"   Top 10 Most Important Features:")
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
        print(f"   {i+1:2d}. {row['feature']:<30}: {row['importance']:.4f}")

# Technical achievements
print(f"\nüîß SUPER TECHNICAL ACHIEVEMENTS:")
print(f"   ‚úÖ Advanced outlier detection with Isolation Forest")
print(f"   ‚úÖ Log transformation reduced price skewness significantly")
print(f"   ‚úÖ {len(valid_feature_columns)} engineered features including:")
print(f"      ‚Ä¢ Geographic intelligence (postcode analysis, market indicators)")
print(f"      ‚Ä¢ Temporal sophistication (rolling stats, momentum, seasonality)")
print(f"      ‚Ä¢ Advanced interactions (property√ólocation√ótime)")
print(f"      ‚Ä¢ Target encoding for high-cardinality categories")
print(f"   ‚úÖ Hyperparameter optimization with Optuna")
print(f"   ‚úÖ Time-aware validation respecting temporal structure")
print(f"   ‚úÖ State-of-art models: XGBoost, LightGBM, CatBoost")
print(f"   ‚úÖ Ensemble methods: Voting and Stacking")

# Business impact
print(f"\nüíº BUSINESS IMPACT:")
print(f"   ‚Ä¢ Prediction Accuracy: ¬±¬£{current_best_mae:,.0f} average error")
print(f"   ‚Ä¢ Variance Explained: {current_best_r2:.1%} of price variation")
print(f"   ‚Ä¢ Model Reliability: {results[most_stable]['cv_mean']:.4f} ¬± {results[most_stable]['cv_std']:.4f}")
print(f"   ‚Ä¢ Processing Speed: {len(df_clean):,} records in real-time")

# Recommendations
print(f"\nüí° SUPER RECOMMENDATIONS:")
print(f"      ‚Ä¢ Use {best_overall} for maximum accuracy")
print(f"      ‚Ä¢ Implement {most_stable} for production stability")
print(f"      ‚Ä¢ Deploy ensemble models for critical valuations")
print(f"   üîÑ CONTINUOUS IMPROVEMENT:")
print(f"      ‚Ä¢ Retrain monthly with new data")
print(f"      ‚Ä¢ Monitor feature importance shifts")
print(f"      ‚Ä¢ A/B test ensemble vs single models")
print(f"   üìä FURTHER ENHANCEMENTS:")
print(f"      ‚Ä¢ External economic indicators (interest rates, GDP)")
print(f"      ‚Ä¢ Property characteristics (size, bedrooms, condition)")
print(f"      ‚Ä¢ Local amenities (schools, transport, crime rates)")
print(f"      ‚Ä¢ Real-time market sentiment data")

print(f"\n‚úÖ SUPER PIPELINE COMPLETED SUCCESSFULLY!")
print(f"   üìä Visualizations saved as 'uk_house_price_prediction_SUPER.png'")
print(f"   üéØ Target Achievement: R¬≤ = {current_best_r2:.4f}, MAE = ¬£{current_best_mae:,.0f}")
print("=" * 90)

